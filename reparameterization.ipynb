{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from colorama import init, Fore\n",
    "init(autoreset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ResNetBasicblock class is defined.\n"
     ]
    }
   ],
   "source": [
    "class ResNetBasicblock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, Ifbias=False, stride=1, downsample=None):\n",
    "        super(ResNetBasicblock, self).__init__()\n",
    "\n",
    "        self.conv_a = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=Ifbias)\n",
    "        self.bn_a = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv_b = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=Ifbias)\n",
    "        self.bn_b = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m.kernel_size != (1, 1):\n",
    "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        basicblock = self.conv_a(x)\n",
    "        basicblock = self.bn_a(basicblock)\n",
    "        basicblock = F.relu(basicblock, inplace=True)\n",
    "\n",
    "        basicblock = self.conv_b(basicblock)\n",
    "        basicblock = self.bn_b(basicblock)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        return F.relu(residual + basicblock, inplace=True)\n",
    "    \n",
    "print(\"The ResNetBasicblock class is defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three structural reparameterization methods are implemented in this file: \n",
      "       ST_Structural_Transformation, SE_Structural_Expansion, and SC_Structural_Compression.\n"
     ]
    }
   ],
   "source": [
    "def ST_Structural_Transformation(BasicBlock):\n",
    "    def Conv_BN(Conv, BN):\n",
    "        gamma = BN.weight\n",
    "        std = (BN.running_var + BN.eps).sqrt()\n",
    "        kernel = Conv.weight\n",
    "        if Conv.bias is not None:\n",
    "            b1 = torch.matmul(torch.eye(kernel.shape[0]).to(kernel.device)*(gamma / std), Conv.bias).to(kernel.device)\n",
    "            return kernel * ((gamma / std).reshape(-1, 1, 1, 1)), BN.bias - BN.running_mean* gamma / std + b1\n",
    "        else:\n",
    "            return kernel * ((gamma / std).reshape(-1, 1, 1, 1)), BN.bias - BN.running_mean * gamma / std\n",
    "    a_inplanes = BasicBlock.conv_a.in_channels\n",
    "    a_planes = BasicBlock.conv_a.out_channels\n",
    "    a_stride = BasicBlock.conv_a.stride\n",
    "    b_inplanes = BasicBlock.conv_b.in_channels\n",
    "    b_planes = BasicBlock.conv_b.out_channels\n",
    "    b_stride = BasicBlock.conv_b.stride\n",
    "    new_conv_a_weight, new_conv_a_bias = Conv_BN(BasicBlock.conv_a, BasicBlock.bn_a)\n",
    "    new_conv_b_weight, new_conv_b_bias = Conv_BN(BasicBlock.conv_b, BasicBlock.bn_b)\n",
    "    BasicBlock.conv_a = nn.Conv2d(a_inplanes, a_planes, kernel_size=3, stride=a_stride, padding=1, bias=True)\n",
    "    BasicBlock.conv_b = nn.Conv2d(b_inplanes, b_planes, kernel_size=3, stride=b_stride, padding=1, bias=True)\n",
    "    BasicBlock.conv_a.weight.data = new_conv_a_weight\n",
    "    BasicBlock.conv_a.bias.data = new_conv_a_bias\n",
    "    BasicBlock.conv_b.weight.data = new_conv_b_weight\n",
    "    BasicBlock.conv_b.bias.data = new_conv_b_bias\n",
    "    return BasicBlock\n",
    "\n",
    "def SE_Structural_Expansion(BasicBlock):\n",
    "    class LDE(nn.Module):\n",
    "        def __init__(self, BasicBlock):\n",
    "            super(LDE, self).__init__()\n",
    "            \n",
    "\n",
    "            # The new pathway\n",
    "            self.conv_a_2 = nn.Conv2d(BasicBlock.conv_a.in_channels, BasicBlock.conv_a.out_channels, \n",
    "                                      kernel_size=BasicBlock.conv_a.kernel_size, stride=BasicBlock.conv_a.stride, \n",
    "                                      padding=BasicBlock.conv_a.padding, bias=False)\n",
    "            self.bn_a_2 = nn.BatchNorm2d(BasicBlock.conv_a.out_channels)\n",
    "            self.conv_b_2 = nn.Conv2d(BasicBlock.conv_b.in_channels, BasicBlock.conv_b.out_channels, \n",
    "                                      kernel_size=BasicBlock.conv_b.kernel_size, stride=BasicBlock.conv_b.stride, \n",
    "                                      padding=BasicBlock.conv_b.padding, bias=False)\n",
    "            self.bn_b_2 = nn.BatchNorm2d(BasicBlock.conv_b.out_channels)\n",
    "            self.bn_a_1 = nn.BatchNorm2d(BasicBlock.conv_a.out_channels)\n",
    "            self.bn_b_1 = nn.BatchNorm2d(BasicBlock.conv_b.out_channels)\n",
    "\n",
    "            # The Fusion Selectors\n",
    "            self.fs_a_1 = nn.Conv2d(BasicBlock.conv_a.out_channels, BasicBlock.conv_a.out_channels, \n",
    "                                    kernel_size=1, bias=False)\n",
    "            self.fs_a_2 = nn.Conv2d(BasicBlock.conv_a.out_channels, BasicBlock.conv_a.out_channels,\n",
    "                                    kernel_size=1, bias=False)\n",
    "            self.fs_b_1 = nn.Conv2d(BasicBlock.conv_b.out_channels, BasicBlock.conv_b.out_channels, \n",
    "                                    kernel_size=1, bias=False)\n",
    "            self.fs_b_2 = nn.Conv2d(BasicBlock.conv_b.out_channels, BasicBlock.conv_b.out_channels,\n",
    "                                    kernel_size=1, bias=False)\n",
    "            \n",
    "            # Initialize the weights of the New Pathway and Fusion Selectors\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    if m.kernel_size != (1, 1):\n",
    "                        m.weight.data.fill_(0)\n",
    "                    if m.kernel_size == (1, 1):\n",
    "                        feature_num = m.weight.size(0)\n",
    "                        identity_mat = np.eye(feature_num, dtype=np.float32)\n",
    "                        m.weight.data = torch.from_numpy(identity_mat).reshape(feature_num, feature_num, 1, 1)\n",
    "                        # m.bias.data.zero_()\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    m.weight.data.fill_(1)\n",
    "                    m.bias.data.zero_()\n",
    "            \n",
    "            # The old pathway\n",
    "            self.conv_a_1 = BasicBlock.conv_a\n",
    "            self.conv_b_1 = BasicBlock.conv_b\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            residual = x\n",
    "\n",
    "            # Block 1\n",
    "            x1up = self.bn_a_1(self.conv_a_1(x))\n",
    "            x1up = self.fs_a_1(x1up)\n",
    "            x1down = self.bn_a_2(self.conv_a_2(x))\n",
    "            x1down = self.fs_a_2(x1down)\n",
    "            x1f = x1up + x1down\n",
    "\n",
    "            x1f = F.relu(x1f)\n",
    "\n",
    "            # Block 2\n",
    "            x2up = self.bn_b_1(self.conv_b_1(x1f))\n",
    "            x2up = self.fs_b_1(x2up)\n",
    "            x2down = self.bn_b_2(self.conv_b_2(x1f))\n",
    "            x2down = self.fs_b_2(x2down)\n",
    "            x2f = x2up + x2down\n",
    "\n",
    "            return F.relu(residual + x2f)\n",
    "    \n",
    "    return LDE(BasicBlock)\n",
    "\n",
    "def SC_Structural_Compression(LDEBolck):\n",
    "    def Conv_BN(Conv, BN):\n",
    "        gamma = BN.weight\n",
    "        std = (BN.running_var + BN.eps).sqrt()\n",
    "        kernel = Conv.weight\n",
    "        if Conv.bias is not None:\n",
    "            b1 = torch.matmul(torch.eye(kernel.shape[0]).to(kernel.device)*(gamma / std), Conv.bias).to(kernel.device)\n",
    "            return kernel * ((gamma / std).reshape(-1, 1, 1, 1)), BN.bias - BN.running_mean* gamma / std + b1\n",
    "        else:\n",
    "            return kernel * ((gamma / std).reshape(-1, 1, 1, 1)), BN.bias - BN.running_mean * gamma / std\n",
    "    def Conv_fs(k1, b1, k2):\n",
    "        k = F.conv2d(k1.permute(1, 0, 2, 3), k2, padding=0).permute(1, 0, 2, 3)\n",
    "        b = torch.matmul(k2.squeeze(2).squeeze(2) , b1)\n",
    "        return k, b\n",
    "\n",
    "    Original_Block = ResNetBasicblock(LDEBolck.conv_a_1.in_channels, LDEBolck.conv_a_1.out_channels, True)\n",
    "\n",
    "    a_1_weight_temp, a_1_bias_temp = Conv_BN(LDEBolck.conv_a_1, LDEBolck.bn_a_1)\n",
    "    a_1_weight, a_1_bias = Conv_fs(a_1_weight_temp, a_1_bias_temp, LDEBolck.fs_a_1.weight)\n",
    "    a_2_weight_temp, a_2_bias_temp = Conv_BN(LDEBolck.conv_a_2, LDEBolck.bn_a_2)\n",
    "    a_2_weight, a_2_bias = Conv_fs(a_2_weight_temp, a_2_bias_temp, LDEBolck.fs_a_2.weight)\n",
    "    Original_Block.conv_a = nn.Conv2d(LDEBolck.conv_a_1.in_channels, LDEBolck.conv_a_1.out_channels,\n",
    "                                      kernel_size=LDEBolck.conv_a_1.kernel_size, stride=LDEBolck.conv_a_1.stride,\n",
    "                                      padding=LDEBolck.conv_a_1.padding, groups=LDEBolck.conv_a_1.groups, bias=True)\n",
    "    \n",
    "    Original_Block.conv_a.weight.data = sum((a_1_weight, a_2_weight))\n",
    "    Original_Block.conv_a.bias.data = sum(a_1_bias,a_2_bias)\n",
    "    Original_Block.bn_a = nn.BatchNorm2d(LDEBolck.conv_a_1.out_channels)\n",
    "    Original_Block.bn_a.weight.data.fill_(1)\n",
    "    Original_Block.bn_a.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "    b_1_weight_temp, b_1_bias_temp = Conv_BN(LDEBolck.conv_b_1, LDEBolck.bn_b_1)\n",
    "    b_1_weight, b_1_bias = Conv_fs(b_1_weight_temp, b_1_bias_temp, LDEBolck.fs_b_1.weight)\n",
    "    b_2_weight_temp, b_2_bias_temp = Conv_BN(LDEBolck.conv_b_2, LDEBolck.bn_b_2)\n",
    "    b_2_weight, b_2_bias = Conv_fs(b_2_weight_temp, b_2_bias_temp, LDEBolck.fs_b_2.weight)\n",
    "    Original_Block.conv_b = nn.Conv2d(LDEBolck.conv_b_1.in_channels, LDEBolck.conv_b_1.out_channels,\n",
    "                                      kernel_size=LDEBolck.conv_b_1.kernel_size, stride=LDEBolck.conv_b_1.stride,\n",
    "                                      padding=LDEBolck.conv_b_1.padding, groups=LDEBolck.conv_b_1.groups, bias=True)\n",
    "    \n",
    "    Original_Block.conv_b.weight.data = sum((b_1_weight, b_2_weight))\n",
    "    Original_Block.conv_b.bias.data = sum(b_1_bias,b_2_bias)\n",
    "    Original_Block.bn_b = nn.BatchNorm2d(LDEBolck.conv_b_1.out_channels)\n",
    "    Original_Block.bn_b.weight.data.fill_(1)\n",
    "    Original_Block.bn_b.bias.data.fill_(0)\n",
    "\n",
    "    return Original_Block\n",
    "\n",
    "print(\"Three structural reparameterization methods are implemented in this file: \\n \\\n",
    "      ST_Structural_Transformation, SE_Structural_Expansion, and SC_Structural_Compression.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output loss After ST:\n",
      "2.644e-08\n",
      "which is less than 1e-7, indicating that it is lossless.\n"
     ]
    }
   ],
   "source": [
    "# Define Random Input\n",
    "inplane = 64\n",
    "outplane = 64\n",
    "batch_size = 1\n",
    "input = torch.randn(batch_size, inplane, 32, 32)\n",
    "\n",
    "Initial_Structure = ResNetBasicblock(inplane, outplane)\n",
    "\n",
    "output_1 = Initial_Structure(input)\n",
    "\n",
    "# Apply ST to the initial structure\n",
    "Structural_after_ST = ST_Structural_Transformation(Initial_Structure)\n",
    "\n",
    "output_2 = Structural_after_ST(input)\n",
    "print(\"Output loss After ST:\")\n",
    "print(Fore.RED+\"{:.3e}\".format(((output_1 - output_2)**2).sum().data.item()))\n",
    "print(\"which is less than 1e-7, indicating that it is lossless.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output loss After SE:\n",
      "0.000e+00\n",
      "which is less than 1e-7, indicating that it is lossless.\n"
     ]
    }
   ],
   "source": [
    "# Apply SE to block\n",
    "Structural_after_SE = SE_Structural_Expansion(Structural_after_ST)\n",
    "\n",
    "output_3 = Structural_after_SE(input)\n",
    "print(\"Output loss After SE:\")\n",
    "print(Fore.RED+\"{:.3e}\".format(((output_2 - output_3)**2).sum().data.item()))\n",
    "print(\"which is less than 1e-7, indicating that it is lossless.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output loss After SC:\n",
      "3.903e-08\n",
      "which is less than 1e-7, indicating that it is lossless.\n",
      "Verify whether the structure after SC is the same as the original network structure.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Apply SC to the LDE block\n",
    "Structural_after_SC = SC_Structural_Compression(Structural_after_SE)\n",
    "\n",
    "output_4 = Structural_after_SC(input)\n",
    "\n",
    "print(\"Output loss After SC:\")\n",
    "print(Fore.RED+\"{:.3e}\".format(((output_3 - output_4)**2).sum().data.item()))\n",
    "print(\"which is less than 1e-7, indicating that it is lossless.\")\n",
    "print(\"Verify whether the structure after SC is the same as the original network structure.\")\n",
    "print(str(Initial_Structure)==str(Structural_after_SC))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xuandi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
